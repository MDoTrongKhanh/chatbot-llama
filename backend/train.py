import torch
from transformers import AutoTokenizer
from datasets import load_dataset
from model import model

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# T·∫£i d·ªØ li·ªáu hu·∫•n luy·ªán t·ª´ Hugging Face (OpenWebText)
dataset = load_dataset("openwebtext", split="train")

# Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
def tokenize_data(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

dataset = dataset.map(tokenize_data, batched=True)

# Hu·∫•n luy·ªán m√¥ h√¨nh
optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)
loss_fn = torch.nn.CrossEntropyLoss()

for epoch in range(5):
    for batch in dataset:
        inputs = torch.tensor(batch["input_ids"])
        targets = torch.tensor(batch["input_ids"])

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = loss_fn(outputs.view(-1, 50000), targets.view(-1))
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch+1}: Loss = {loss.item()}")

# L∆∞u m√¥ h√¨nh
torch.save(model.state_dict(), "models/chatbot_model.pth")
print("üéâ M√¥ h√¨nh ƒë√£ train xong v√† l∆∞u v√†o models/chatbot_model.pth")
